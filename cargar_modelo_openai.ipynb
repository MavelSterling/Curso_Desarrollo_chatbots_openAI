{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Instalar la libreria"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWRLbJinfIm-",
        "outputId": "3af4af0f-e72a-4586-c6ae-834a16896cae"
      },
      "outputs": [],
      "source": [
        "pip install openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Importar la libreria"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JMiHUAbSgmsS"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Key de OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sN5zFG35goyz"
      },
      "outputs": [],
      "source": [
        "openai = OpenAI(api_key=\"Ingresa tu API Key de OpenAI\") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## **Qué hace esta celda:**\n",
        "\n",
        "1. Llama al endpoint de **chat completions**:\n",
        "\n",
        "   * `model='gpt-3.5-turbo'`: selecciona el modelo.\n",
        "   * `messages`: envía el contexto como conversación:\n",
        "\n",
        "     * `system` define el rol/tono del asistente.\n",
        "     * `user` contiene la pregunta.\n",
        "   * `max_tokens=50`: limita la longitud máxima aproximada de la respuesta (en tokens).\n",
        "\n",
        "2. Guarda el resultado en `response`.\n",
        "   Dentro de `response` viene una lista `choices` con las respuestas generadas.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNKHA3Phgws3",
        "outputId": "7201bdec-ae38-43af-891d-ee86ed45384c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cristóbal Colón es generalmente acreditado como el descubridor de América. En 1492, Colón, bajo el patrocinio de los Reyes Católicos de España, realizó su famoso viaje\n"
          ]
        }
      ],
      "source": [
        "response = openai.chat.completions.create(\n",
        "    model = 'gpt-3.5-turbo',\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\" :\"Eres un asistente que da informacion a dudas\"},\n",
        "        {\"role\": \"user\", \"content\" :\"¿Quién descubrió América?\"}\n",
        "    ],\n",
        "    max_tokens = 50,\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9253KN3KNJKf",
        "outputId": "e2264882-20ea-4d55-bd6a-0b2ebf2bfcb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatCompletion(id='chatcmpl-8QHQcA5LcbVEyahSKBYakCIBnZHAR', choices=[Choice(finish_reason='length', index=0, message=ChatCompletionMessage(content='El descubrimiento de América generalmente se atribuye a Cristóbal Colón, un navegante y explorador genovés que sirvió a los Reyes Católicos de España. Colón llegó a las Américas', role='assistant', function_call=None, tool_calls=None)), Choice(finish_reason='length', index=1, message=ChatCompletionMessage(content='El explorador Cristóbal Colón es conocido por haber descubierto América en 1492. Colón, un navegante genovés al servicio de los Reyes Católicos de España, llegó a las Américas mientras', role='assistant', function_call=None, tool_calls=None))], created=1701274214, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=100, prompt_tokens=31, total_tokens=131))\n"
          ]
        }
      ],
      "source": [
        "response = openai.chat.completions.create(\n",
        "    model = 'gpt-3.5-turbo',\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\" :\"Eres un asistente que da informacion a dudas\"},\n",
        "        {\"role\": \"user\", \"content\" :\"¿Quién descubrió América?\"}\n",
        "    ],\n",
        "    max_tokens = 50,\n",
        "    temperature = 1,\n",
        "    top_p = 1,\n",
        "    n = 2\n",
        ")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "El mundial de fútbol de 2022 se jugó en Qatar. Fue la primera vez que este país de Oriente Medio fue sede de la Copa del Mundo. Los partidos se llevaron a cabo en ocho est\n"
          ]
        }
      ],
      "source": [
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "El mundial de fútbol de 2022 se jugó en Qatar. Fue la primera vez que se llevó a cabo en ese país.\n"
          ]
        }
      ],
      "source": [
        "print(response.choices[1].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## **Qué agrega esta celda respecto a la anterior:**\n",
        "\n",
        "* `temperature=0.3`: baja creatividad/variación → respuestas más consistentes y “serias”.\n",
        "* `top_p=1`: deja el muestreo completo (si lo bajas, también reduce variedad).\n",
        "* `n=2`: pide **dos** respuestas alternativas. Por eso `response.choices` tendrá dos elementos: `choices[0]` y `choices[1]`.\n",
        "* `max_tokens=100`: permite respuestas más largas que en la celda 6.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = openai.chat.completions.create(\n",
        "    model = 'gpt-3.5-turbo',\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\" :\"Eres un asistente que da informacion a dudas\"},\n",
        "        {\"role\": \"user\", \"content\" :\"¿Qué es IA?\"}\n",
        "    ],\n",
        "    max_tokens = 100,\n",
        "    temperature = 0.3,\n",
        "    top_p = 1,\n",
        "    n = 2\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ejemplo practico"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* **Variables de entorno**: evita pegar la API Key directamente en el notebook (mejor práctica).\n",
        "* **`messages` con varios turnos**: al incluir conversación previa, el modelo entiende el contexto.\n",
        "* **`temperature=0.7`**: genera respuestas más variadas que 0.2–0.3.\n",
        "* **`n=2`**: solicita dos versiones de respuesta para comparar estilo o nivel de detalle.\n",
        "* **Bucle `for`**: imprime todas las alternativas devueltas en `response.choices`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "# Opción recomendada: guarda la API Key en una variable de entorno\n",
        "# En Windows (PowerShell):  $env:OPENAI_API_KEY=\"tu_api_key\"\n",
        "# En Linux/Mac:            export OPENAI_API_KEY=\"tu_api_key\"\n",
        "\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"Eres un asistente experto en deportes. Responde claro y breve.\"},\n",
        "    {\"role\": \"user\", \"content\": \"¿Quién ganó el mundial de fútbol de 2022?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"El mundial de 2022 lo ganó Argentina.\"},\n",
        "    {\"role\": \"user\", \"content\": \"¿En qué país se jugó y en qué año fue?\"}\n",
        "]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=messages,\n",
        "    temperature=0.7,\n",
        "    max_tokens=120,\n",
        "    n=2\n",
        ")\n",
        "\n",
        "# Imprimir las dos alternativas\n",
        "for i, choice in enumerate(response.choices):\n",
        "    print(f\"Respuesta {i+1}:\\n{choice.message.content}\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
