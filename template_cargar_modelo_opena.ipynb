{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Instalar la libreria"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWRLbJinfIm-",
        "outputId": "3af4af0f-e72a-4586-c6ae-834a16896cae"
      },
      "outputs": [],
      "source": [
        "pip install openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Importa la clase OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JMiHUAbSgmsS"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Crea una instancia del cliente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sN5zFG35goyz"
      },
      "outputs": [],
      "source": [
        "openai = OpenAI(api_key=\"Ingresa tu API Key de OpenAI\") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Qué hace:**\n",
        "\n",
        "1. Llama al endpoint de **chat completions**:\n",
        "\n",
        "   * `model='gpt-3.5-turbo'`: define el modelo.\n",
        "   * `messages=[...]`: define la conversación:\n",
        "\n",
        "     * `system`: le dice al modelo cómo comportarse (“asistente que da información…”).\n",
        "     * `user`: la pregunta del usuario.\n",
        "   * `max_tokens=50`: limita la longitud de la respuesta (aprox. cantidad de “palabras/pedazos” que puede generar; técnicamente son tokens).\n",
        "\n",
        "2. Guarda la respuesta en `response`.\n",
        "\n",
        "3. Imprime el texto final con:\n",
        "\n",
        "   * `response.choices[0].message.content`\n",
        "   * `choices[0]` es la primera (y usualmente la única) respuesta generada.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNKHA3Phgws3",
        "outputId": "7201bdec-ae38-43af-891d-ee86ed45384c"
      },
      "outputs": [],
      "source": [
        "response = openai.chat.completions.create(\n",
        "    model = 'gpt-3.5-turbo',\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\" :\"Eres un asistente que da informacion a dudas\"},\n",
        "        {\"role\": \"user\", \"content\" :\"¿Quién descubrió América?\"}\n",
        "    ],\n",
        "    max_tokens = 50,\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ejemplo practico"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "# Recomendado: guardar la API Key como variable de entorno\n",
        "# En Windows PowerShell:  $env:OPENAI_API_KEY=\"tu_api_key\"\n",
        "# En Linux/Mac:           export OPENAI_API_KEY=\"tu_api_key\"\n",
        "\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"Eres un asistente experto en deportes. Responde claro y con datos puntuales.\"},\n",
        "        {\"role\": \"user\", \"content\": \"¿Quién ganó el mundial de fútbol de 2022 y en qué país se jugó?\"}\n",
        "    ],\n",
        "    temperature=0.7,   # más variación que 0.2–0.3\n",
        "    max_tokens=120,\n",
        "    n=2                # pide 2 alternativas de respuesta\n",
        ")\n",
        "\n",
        "for i, choice in enumerate(response.choices, start=1):\n",
        "    print(f\"Respuesta {i}:\\n{choice.message.content}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ejemplo 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "stream = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"Eres un asistente experto en deportes. Responde claro y breve.\"},\n",
        "        {\"role\": \"user\", \"content\": \"¿En qué país se jugó el mundial de fútbol de 2022?\"}\n",
        "    ],\n",
        "    temperature=0.3,\n",
        "    max_tokens=120,\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "# Imprime el texto a medida que llega (tipo “escribiendo…”)\n",
        "for chunk in stream:\n",
        "    # En streaming, el texto viene por partes (deltas)\n",
        "    delta = chunk.choices[0].delta\n",
        "    if delta and delta.content:\n",
        "        print(delta.content, end=\"\", flush=True)\n",
        "\n",
        "print()  # salto de línea al final\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## ¿Qué hace este código?\n",
        "\n",
        "1) `stream=True`\n",
        "\n",
        "Cuando agregas `stream=True`, el modelo **no devuelve toda la respuesta completa de una sola vez**.\n",
        "En lugar de eso, devuelve un **iterador** (un “flujo”) de fragmentos (chunks) con partes del texto.\n",
        "\n",
        "2) `for chunk in stream:`\n",
        "\n",
        "* Recorre cada fragmento que llega desde la API.\n",
        "* Cada `chunk` trae una porción pequeña de la respuesta.\n",
        "\n",
        "3) `chunk.choices[0].delta.content`\n",
        "\n",
        "* En modo streaming, la salida no está en `message.content` como antes.\n",
        "* Llega en un “delta” (pedacito incremental). Por eso se usa:\n",
        "\n",
        "  * `chunk.choices[0].delta.content`\n",
        "* Algunos chunks no traen texto (pueden traer señales de control), por eso se valida con `if`.\n",
        "\n",
        "4) `print(..., end=\"\", flush=True)`\n",
        "\n",
        "* `end=\"\"` evita que imprima salto de línea en cada fragmento.\n",
        "* `flush=True` fuerza a que se muestre inmediatamente en la salida del notebook.\n",
        "\n",
        "---\n",
        "\n",
        "## Diferencia con el caso normal (`stream=False`)\n",
        "\n",
        "**Sin streaming (normal)**\n",
        "\n",
        "* Esperas a que termine toda la generación.\n",
        "* Accedes al texto así:\n",
        "\n",
        "```python\n",
        "response = client.chat.completions.create(...)\n",
        "print(response.choices[0].message.content)\n",
        "```\n",
        "\n",
        "**Con streaming (`stream=True`)**\n",
        "\n",
        "* Vas recibiendo la respuesta por pedazos.\n",
        "* Accedes al texto así:\n",
        "\n",
        "```python\n",
        "for chunk in stream:\n",
        "    print(chunk.choices[0].delta.content, end=\"\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**¿Cuándo conviene usar `stream=True`?**\n",
        "\n",
        "* Cuando quieres una UX tipo “escribiendo…”.\n",
        "* Cuando las respuestas pueden ser largas y quieres ver progreso.\n",
        "* Cuando quieres empezar a procesar texto antes de que termine (por ejemplo, mostrarlo en interfaz).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ejemplo 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Conceptos clave (qué controla cada parámetro)**\n",
        "\n",
        "**`max_tokens`**\n",
        "\n",
        "* Define la **longitud máxima** de la respuesta generada.\n",
        "* Si lo pones muy bajo, la respuesta puede quedar **cortada** o demasiado breve.\n",
        "* Si lo aumentas, permites respuestas más extensas (aunque no “obligas” al modelo a llenar todo; solo amplías el límite).\n",
        "\n",
        "**Ejemplo:**\n",
        "\n",
        "* `max_tokens=20` → una o dos frases cortas.\n",
        "* `max_tokens=150` → explicación más completa con contexto.\n",
        "\n",
        "---\n",
        "\n",
        "**`temperature`**\n",
        "\n",
        "* Controla la **creatividad/variabilidad** del modelo.\n",
        "* **Baja temperatura (0.0–0.3):** más predecible, consistente, “conservadora”.\n",
        "* **Alta temperatura (0.8–1.2):** más variada, con más libertad de redacción y detalles (y también mayor riesgo de “irse por las ramas” o agregar cosas no solicitadas).\n",
        "\n",
        "**Regla práctica:**\n",
        "\n",
        "* Si quieres respuestas “tipo dato” o muy estables → usa temperatura baja.\n",
        "* Si quieres alternativas, redacción distinta, ideas → sube la temperatura.\n",
        "\n",
        "---\n",
        "\n",
        "**`top_p`**\n",
        "\n",
        "* Controla la diversidad mediante **nucleus sampling**.\n",
        "* Funciona como un “filtro probabilístico”: el modelo considera solo el conjunto de tokens cuya probabilidad acumulada llega a `top_p`.\n",
        "* **`top_p` bajo (ej. 0.2–0.5):** limita la gama de tokens considerados → menos variabilidad.\n",
        "* **`top_p` alto (cerca de 1):** deja pasar más opciones → mayor diversidad.\n",
        "\n",
        "**Importante:** en la práctica se suele ajustar **temperature o top_p**, no ambos a la vez, para entender claramente el efecto de cada uno.\n",
        "\n",
        "---\n",
        "\n",
        "**`n`**\n",
        "\n",
        "* Indica cuántas **variantes de respuesta** quieres que devuelva el modelo.\n",
        "* Si `n=1`, recibes una sola respuesta.\n",
        "* Si `n=3`, recibes tres alternativas en `response.choices`.\n",
        "\n",
        "Esto es muy útil cuando:\n",
        "\n",
        "* quieres comparar estilos,\n",
        "* buscas diferentes formas de explicar lo mismo,\n",
        "* o deseas escoger la “mejor” respuesta entre varias.\n",
        "\n",
        "---\n",
        "\n",
        "**¿Qué deberías observar en los resultados?**\n",
        "\n",
        "1) Efecto de `max_tokens`\n",
        "\n",
        "- **Caso 1 (max_tokens=25):**\n",
        "\n",
        "  * Probablemente una respuesta de una sola línea.\n",
        "  * Ejemplo esperado: “Cristóbal Colón en 1492.”\n",
        "  * Si el modelo intenta añadir contexto, puede cortarse.\n",
        "\n",
        "-  **Caso 2 (max_tokens=150):**\n",
        "\n",
        "  * Puede explicar con más detalle:\n",
        "\n",
        "    * quién fue Colón,\n",
        "    * año,\n",
        "    * patrocinio (Corona de Castilla),\n",
        "    * matices como “llegada a América” vs “descubrimiento” (enfoque histórico).\n",
        "\n",
        "**Idea importante:** `max_tokens` no cambia “la verdad”, cambia el *espacio disponible* para explicarla.\n",
        "\n",
        "---\n",
        "\n",
        "2) Efecto de `temperature`\n",
        "\n",
        "- **Caso 3 (temperature=0.1):**\n",
        "\n",
        "  * Respuesta más directa, parecida entre ejecuciones repetidas.\n",
        "  * Poca variación en palabras.\n",
        "\n",
        "- **Caso 4 (temperature=1.0):**\n",
        "\n",
        "  * Puede cambiar la redacción entre ejecuciones:\n",
        "\n",
        "    * “Cristóbal Colón llegó a América en 1492…”\n",
        "    * “El viaje liderado por Colón…”\n",
        "    * puede incluir más contexto o matices, incluso si no lo pediste.\n",
        "  * Mayor diversidad en cómo explica.\n",
        "\n",
        "**Resumen:**\n",
        "\n",
        "* Baja temperatura: coherente, conservadora.\n",
        "* Alta temperatura: variada, menos predecible.\n",
        "\n",
        "---\n",
        "\n",
        "3) Efecto de `top_p`\n",
        "\n",
        "* **Caso 5 (top_p=0.3):**\n",
        "\n",
        "  * El modelo se limita a tokens “muy probables”.\n",
        "  * Normalmente verás menos creatividad en vocabulario y menos caminos alternativos.\n",
        "  * La respuesta suele ser más “recta”, con menos adornos.\n",
        "\n",
        "**¿Qué cambia al alterar top_p?**\n",
        "\n",
        "* **top_p bajo**: reduce la gama de salidas posibles, por lo tanto baja variabilidad.\n",
        "* **top_p alto (1.0)**: permite más diversidad (siempre en combinación con el muestreo).\n",
        "\n",
        "---\n",
        "\n",
        " 4) Efecto de `n`\n",
        "\n",
        "* **Caso 6 (n=3):**\n",
        "\n",
        "  * El modelo devuelve 3 alternativas.\n",
        "  * Puede que las tres digan el mismo dato principal, pero con:\n",
        "\n",
        "    * diferente estilo (más breve vs más explicativo),\n",
        "    * distinto enfoque (histórico, educativo, neutral),\n",
        "    * o variaciones de redacción.\n",
        "\n",
        "Esto es útil para:\n",
        "\n",
        "* elegir la respuesta que mejor encaje con tu audiencia,\n",
        "* comparar consistencia,\n",
        "* o validar si el modelo mantiene el dato central en varias opciones.\n",
        "\n",
        "---\n",
        "\n",
        "**Recomendaciones prácticas**\n",
        "\n",
        "- Si quieres respuestas **consistentes y tipo “dato”**:\n",
        "\n",
        "  * `temperature=0.0–0.3`\n",
        "  * `top_p=1.0` (o no tocarlo)\n",
        "  * `n=1`\n",
        "  * `max_tokens` ajustado al tamaño esperado (ej. 60–120)\n",
        "\n",
        "- Si quieres **varias alternativas** para escoger:\n",
        "\n",
        "  * `n=3` (o más, con moderación)\n",
        "  * `temperature=0.7` (para que no sean casi iguales)\n",
        "  * `max_tokens` suficiente para que haya diferencias reales\n",
        "\n",
        "- Si notas demasiada variación o “adornos”:\n",
        "\n",
        "  * baja `temperature` **o** baja `top_p`\n",
        "  * (mejor cambiar uno solo para ver claramente el efecto)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "def run_case(title, max_tokens=80, temperature=0.2, top_p=1.0, n=1):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"Eres un asistente educativo. Responde con claridad y precisión.\"},\n",
        "            {\"role\": \"user\", \"content\": \"¿Quién descubrió América?\"}\n",
        "        ],\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        n=n\n",
        "    )\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"{title}\")\n",
        "    print(f\"max_tokens={max_tokens} | temperature={temperature} | top_p={top_p} | n={n}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    for i, choice in enumerate(response.choices, start=1):\n",
        "        print(f\"[Respuesta {i}] {choice.message.content}\\n\")\n",
        "\n",
        "\n",
        "# Caso 1: Respuesta corta (max_tokens bajo)\n",
        "run_case(\n",
        "    title=\"Caso 1: max_tokens bajo (respuesta corta)\",\n",
        "    max_tokens=25,\n",
        "    temperature=0.2,\n",
        "    top_p=1.0,\n",
        "    n=1\n",
        ")\n",
        "\n",
        "# Caso 2: Respuesta más larga (max_tokens alto)\n",
        "run_case(\n",
        "    title=\"Caso 2: max_tokens alto (más explicación)\",\n",
        "    max_tokens=150,\n",
        "    temperature=0.2,\n",
        "    top_p=1.0,\n",
        "    n=1\n",
        ")\n",
        "\n",
        "# Caso 3: Temperatura baja (más estable)\n",
        "run_case(\n",
        "    title=\"Caso 3: temperatura baja (más predecible)\",\n",
        "    max_tokens=120,\n",
        "    temperature=0.1,\n",
        "    top_p=1.0,\n",
        "    n=1\n",
        ")\n",
        "\n",
        "# Caso 4: Temperatura alta (más variabilidad)\n",
        "run_case(\n",
        "    title=\"Caso 4: temperatura alta (más variación en estilo y detalle)\",\n",
        "    max_tokens=120,\n",
        "    temperature=1.0,\n",
        "    top_p=1.0,\n",
        "    n=1\n",
        ")\n",
        "\n",
        "# Caso 5: top_p bajo (diversidad restringida) con temperatura moderada\n",
        "run_case(\n",
        "    title=\"Caso 5: top_p bajo (reduce la diversidad del texto)\",\n",
        "    max_tokens=120,\n",
        "    temperature=0.7,\n",
        "    top_p=0.3,\n",
        "    n=1\n",
        ")\n",
        "\n",
        "# Caso 6: Variantes múltiples (n > 1)\n",
        "run_case(\n",
        "    title=\"Caso 6: n=3 (tres alternativas para elegir)\",\n",
        "    max_tokens=120,\n",
        "    temperature=0.7,\n",
        "    top_p=1.0,\n",
        "    n=3\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
