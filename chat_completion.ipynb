{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Instalar la libreria"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjgpgG8UX8Yv",
        "outputId": "a1876dae-0d54-4df8-e27a-6ebe847d1d6a"
      },
      "outputs": [],
      "source": [
        "pip install openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# importar la libreria"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RIUa1YQ4X_-o"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "openai = OpenAI(api_key='Ingresa aquí tu API Key de OpenAI')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# chat completions\n",
        "\n",
        "¿Qué son las chat completions y cómo funcionan?\n",
        "\n",
        "Las chat completions son una forma más avanzada de interactuar con modelos de lenguaje porque trabajan con una conversación estructurada, no con un único prompt aislado.\n",
        "\n",
        "A diferencia de las completions tradicionales (una entrada → una salida), en chat completions tú envías una lista de mensajes que representan el historial del diálogo. Ese historial le da al modelo contexto, lo ayuda a entender qué se está preguntando ahora y qué se dijo antes, y permite respuestas más coherentes y alineadas con lo que necesitas.\n",
        "\n",
        "---\n",
        "\n",
        "¿Cómo se estructuran los roles en las chat completions?\n",
        "\n",
        "En una conversación, cada mensaje lleva un role que indica quién “habla” y con qué intención:\n",
        "\n",
        "- system: define el comportamiento general del asistente (reglas, tono, rol profesional, límites, etc.).\n",
        "Ej: “Eres un asistente experto en deportes”.\n",
        "\n",
        "- user: representa lo que pregunta o solicita la persona usuaria.\n",
        "\n",
        "- assistant: representa respuestas anteriores del asistente (puedes incluirlas para mantener continuidad o para “anclar” el contexto con ejemplos).\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **¿Qué hace cada parte?**\n",
        "\n",
        "1. **`openai.chat.completions.create(...)`**\n",
        "   Llama al modelo para que genere la siguiente respuesta del chat, basándose en el historial.\n",
        "\n",
        "2. **`model='gpt-3.5-turbo'`**\n",
        "   Indica qué modelo usar para generar la respuesta.\n",
        "\n",
        "3. **`messages=[...]`**\n",
        "   Es el **historial de conversación** en orden:\n",
        "\n",
        "   * `system`: fija el rol (“asistente de deportes”).\n",
        "   * `user`: pregunta 1: “¿Quién ganó el mundial…?”\n",
        "   * `assistant`: respuesta previa (la incluyes para dar continuidad explícita).\n",
        "   * `user`: pregunta 2: “¿Dónde se jugó?”\n",
        "     Como el historial incluye que se habla del **Mundial 2022**, el modelo entiende que “¿Dónde se jugó?” se refiere a ese mundial.\n",
        "\n",
        "4. **`response`**\n",
        "   Guarda la respuesta completa del modelo (incluye metadatos, opciones, etc.).\n",
        "\n",
        "5. **`response.choices[0].message.content`**\n",
        "   Extrae el texto final de la respuesta generada por el asistente (la “mejor” opción devuelta en la primera posición).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WotauwKmYUBn",
        "outputId": "8260ca9e-665e-48d8-9196-9b21c8f84c81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "El Mundial de Fútbol de 2022 se jugó en Qatar. Fue la primera vez que este país de Medio Oriente organizó el torneo.\n"
          ]
        }
      ],
      "source": [
        "response = openai.chat.completions.create(\n",
        "    model = 'gpt-3.5-turbo',\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\" :\"Eres un asistente que da informacion sobre deportes\"},\n",
        "        {\"role\": \"user\", \"content\" :\"¿Quién ganó el mundial de fútbol?\"},\n",
        "        {\"role\": \"assistant\", \"content\" : \"El mundial de 2022 lo ganó Argentina\"},\n",
        "        {\"role\": \"user\", \"content\" :\"¿Dónde se jugó?\"}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Paso a paso: ¿qué hace cada parte?\n",
        "\n",
        "1. **`openai.chat.completions.create(...)`**\n",
        "\n",
        "   * Envía una solicitud al modelo para generar la **próxima respuesta del asistente** basándose en la conversación (los mensajes).\n",
        "\n",
        "2. **`model='gpt-3.5-turbo'`**\n",
        "\n",
        "   * Define qué modelo se va a usar para generar la respuesta.\n",
        "\n",
        "3. **`messages=[...]` (historial del chat)**\n",
        "\n",
        "   * **`system`**: fija el comportamiento del asistente:\n",
        "\n",
        "     * “Eres un asistente que da información sobre deportes”\n",
        "   * **`user`**: primera pregunta:\n",
        "\n",
        "     * “¿Quién ganó el mundial de fútbol?”\n",
        "   * **`assistant`**: respuesta anterior que tú incluyes para dar contexto:\n",
        "\n",
        "     * “El mundial de 2022 lo ganó Argentina”\n",
        "   * **`user`**: segunda pregunta:\n",
        "\n",
        "     * “¿Dónde se jugó?”\n",
        "   * Gracias a ese historial, el modelo entiende que “¿Dónde se jugó?” se refiere al **Mundial 2022** (no a cualquier mundial).\n",
        "\n",
        "4. **`temperature=0.2`**\n",
        "\n",
        "   * Controla la **variabilidad** de la respuesta:\n",
        "\n",
        "     * Valores **bajos** (como 0.2) → respuestas más **consistentes, directas y menos creativas**.\n",
        "     * Valores **altos** (0.8–1.0) → respuestas más **creativas**, con más variación.\n",
        "   * En este caso, como la pregunta tiene una respuesta factual (“Qatar”), un valor bajo ayuda a que responda **de forma más estable**.\n",
        "\n",
        "5. **`print(response.choices[0].message.content)`**\n",
        "\n",
        "   * Imprime el texto de la respuesta generada por el modelo.\n",
        "   * Normalmente devolvería algo como: **“Se jugó en Qatar.”**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSwaJ1wmbn5z",
        "outputId": "2df6e4c8-a8a3-4fd2-f89a-64fb1a67c9d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "El Mundial de Fútbol de 2022 se jugó en Qatar. Fue la primera vez que este país de Oriente Medio fue sede de la Copa del Mundo.\n"
          ]
        }
      ],
      "source": [
        "response = openai.chat.completions.create(\n",
        "    model = 'gpt-3.5-turbo',\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\" :\"Eres un asistente que da informacion sobre deportes\"},\n",
        "        {\"role\": \"user\", \"content\" :\"¿Quién ganó el mundial de fútbol?\"},\n",
        "        {\"role\": \"assistant\", \"content\" : \"El mundial de 2022 lo ganó Argentina\"},\n",
        "        {\"role\": \"user\", \"content\" :\"¿Dónde se jugó?\"}\n",
        "    ],\n",
        "    temperature = 0.2\n",
        ")\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ¿Qué hace este código?\n",
        "\n",
        "### 1) Llamada al modelo\n",
        "\n",
        "* `openai.chat.completions.create(...)` envía una solicitud al modelo para que genere **la próxima respuesta** del rol `assistant`.\n",
        "* `model='gpt-3.5-turbo'` define qué modelo responde.\n",
        "\n",
        "### 2) Contexto conversacional con `messages`\n",
        "\n",
        "Le entregas al modelo una conversación en orden:\n",
        "\n",
        "* **`system`**: define el comportamiento del asistente.\n",
        "\n",
        "  > “Eres un asistente que da información sobre deportes.”\n",
        "  > Esto guía el tono y el tipo de contenido esperado.\n",
        "\n",
        "* **`user`**: pregunta inicial.\n",
        "\n",
        "  > “¿Quién ganó el mundial de fútbol?”\n",
        "\n",
        "* **`assistant`**: respuesta previa (incluida por ti).\n",
        "\n",
        "  > “El mundial de 2022 lo ganó Argentina”\n",
        "  > Esto “ancla” el contexto: ya no es un mundial genérico, sino el de **2022**.\n",
        "\n",
        "* **`user`**: nueva pregunta dependiente del contexto.\n",
        "\n",
        "  > “¿Dónde se jugó?”\n",
        "  > El modelo interpreta “¿Dónde se jugó?” como “¿Dónde se jugó el Mundial 2022?” gracias a lo anterior.\n",
        "\n",
        "### 3) Control de creatividad con `temperature=1`\n",
        "\n",
        "* `temperature` ajusta cuánta **variación** y **azar** se permite en la generación:\n",
        "\n",
        "  * **Baja** (ej. 0.0–0.3): respuestas más **predecibles**, directas y consistentes.\n",
        "  * **Alta** (ej. 0.8–1.2): respuestas más **variadas**, con más libertad para elegir diferentes palabras, estructuras, detalles e incluso agregar contexto adicional.\n",
        "\n",
        "Con `temperature = 1`, el modelo tiene más margen para:\n",
        "\n",
        "* Expandir la respuesta con detalles (por ejemplo, mencionar que Qatar fue el país anfitrión).\n",
        "* Usar redacciones alternativas: “se celebró en…”, “tuvo lugar en…”, “se disputó en…”\n",
        "* Cambiar el nivel de detalle de una ejecución a otra, aun con la misma pregunta.\n",
        "\n",
        "### 4) Impresión del resultado\n",
        "\n",
        "* `response` contiene la respuesta del modelo y metadatos.\n",
        "* `response.choices[0].message.content` extrae el texto final del **primer candidato** (“choice”) devuelto.\n",
        "* `print(...)` lo muestra en la salida del notebook.\n",
        "\n",
        "---\n",
        "\n",
        "## Diferencias con el código anterior (temperature = 0.2)\n",
        "\n",
        "La estructura del código es esencialmente la misma (mismo modelo, mismos mensajes). La diferencia importante está en el comportamiento esperado de la generación:\n",
        "\n",
        "### 1) Variabilidad de la respuesta\n",
        "\n",
        "* **Antes (0.2):** respuesta más “cerrada”, breve, estable.\n",
        "* **Ahora (1):** respuesta potencialmente más “abierta”, con más variación en estilo y detalle.\n",
        "\n",
        "### 2) Consistencia entre ejecuciones\n",
        "\n",
        "* **Antes (0.2):** si ejecutas varias veces, es más probable que obtengas respuestas casi idénticas.\n",
        "* **Ahora (1):** si ejecutas varias veces, es más probable que el texto cambie (sin cambiar el significado principal).\n",
        "\n",
        "### 3) Riesgo de “inventar” detalles\n",
        "\n",
        "* Con temperaturas más altas, el modelo puede agregar contexto adicional. En preguntas simples suele estar bien, pero en temas sensibles o muy precisos, una temperatura alta puede incrementar el riesgo de:\n",
        "\n",
        "  * introducir detalles no solicitados,\n",
        "  * cometer errores sutiles,\n",
        "  * o sonar más especulativo.\n",
        "    Para respuestas “tipo dato” (fechas, cifras, definiciones estrictas), normalmente conviene **temperaturas bajas**.\n",
        "\n",
        "### 4) Uso típico recomendado\n",
        "\n",
        "* **`temperature` baja:** chatbots de soporte, respuestas consistentes, extracción de información, formato estricto.\n",
        "* **`temperature` alta:** lluvia de ideas, redacción creativa, generación de alternativas, explicaciones más narrativas.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TuU7pdmb9bm",
        "outputId": "0117ff79-5378-442f-b0af-7cccc23c3bfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "El mundial de fútbol de 2022 se llevó a cabo en Qatar.\n"
          ]
        }
      ],
      "source": [
        "response = openai.chat.completions.create(\n",
        "    model = 'gpt-3.5-turbo',\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\" :\"Eres un asistente que da informacion sobre deportes.\"},\n",
        "        {\"role\": \"user\", \"content\" :\"¿Quién ganó el mundial de fútbol?\"},\n",
        "        {\"role\": \"assistant\", \"content\" : \"El mundial de 2022 lo ganó Argentina\"},\n",
        "        {\"role\": \"user\", \"content\" :\"¿Dónde se jugó?\"}\n",
        "    ],\n",
        "    temperature = 1\n",
        ")\n",
        "print(response.choices[0].message.content)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
